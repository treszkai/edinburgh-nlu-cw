import numpy as np


def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))


def softmax(x):
    xt = np.exp(x - max(x))
    return xt / sum(xt)


def grad(x):
    return x * (1 - x)


def make_onehot(i, n):
    y = np.zeros(n)
    y[i] = 1
    return y


def fraq_loss(vocab, word_to_num, vocabsize):
    fraction_lost = float(
        sum([vocab['count'][word] for word in vocab.index if (not word in word_to_num) and (not word == "UUUNKKK")]))
    fraction_lost /= sum([vocab['count'][word] for word in vocab.index if (not word == "UUUNKKK")])
    return fraction_lost


def adjust_loss(loss, fracloss, q, mode='basic'):
    if mode == 'basic':
        # remove freebies only: score if had no UUUNKKK
        return (loss + fracloss * np.log(fracloss)) / (1 - fracloss)
    else:
        # remove freebies, replace with best prediction on remaining
        return loss + fracloss * np.log(fracloss) - fracloss * np.log(q)


class MultinomialSampler(object):
    """
    Fast (O(log n)) sampling from a discrete probability
    distribution, with O(n) set-up time.
    """

    def __init__(self, p, verbose=False):
        n = len(p)
        p = p.astype(float) / sum(p)
        self._cdf = np.cumsum(p)

    def sample(self, k=1):
        rs = np.random.random(k)
        # binary search to get indices
        return np.searchsorted(self._cdf, rs)

    def __call__(self, **kwargs):
        return self.sample(**kwargs)

    def reconstruct_p(self):
        """
        Return the original probability vector.
        Helpful for debugging.
        """
        n = len(self._cdf)
        p = np.zeros(n)
        p[0] = self._cdf[0]
        p[1:] = (self._cdf[1:] - self._cdf[:-1])
        return p


def multinomial_sample(p):
    """
    Wrapper to generate a single sample,
    using the above class.
    """
    return MultinomialSampler(p).sample(1)[0]
